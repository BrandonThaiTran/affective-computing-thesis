{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amigos Audio Classification Tutorial\n",
    "=========================\n",
    "**Author**: Brandon Thai Tran <github.com/BrandonThaiTran>\n",
    "\n",
    "In this notebook, we study the effect of movie audio on affective response in participants using the AMIGOS dataset. For more information on the AMIGOS dataset, please visit the below link.\n",
    "\n",
    "Paper: <http://www.eecs.qmul.ac.uk/mmv/datasets/amigos/doc/Paper_TAC.pdf>\n",
    "\n",
    "Webpage (includes how to download the dataset): http://www.eecs.qmul.ac.uk/mmv/datasets/amigos/index.html\n",
    "\n",
    "The dataset records 40 individuals watch 16 different movie clips while recording modalities during the trial and recording their affective response both before and after the trial. The modalities include face video, body video, depth video, audio, GSR, EEG, and ECG.\n",
    "\n",
    "Before we can run this notebook, we extracted the movie audio from the face videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs\n",
    "------\n",
    "\n",
    "Here are all of the parameters to change for the run. \n",
    "\n",
    "We will write our own custom dataset. Download the data\n",
    "and set the ``data_dir`` input to the root directory of the dataset. \n",
    "\n",
    "The other inputs are as follows: ``num_classes`` is the number of\n",
    "classes in the dataset, ``batch_size`` is the batch size used for\n",
    "training and may be adjusted according to the capability of your\n",
    "machine, ``num_epochs`` is the number of training epochs we want to run, ``feature_extract`` is a boolean that defines if we are finetuning\n",
    "or feature extracting, ``reduce`` is the flag for reducting the training size, and ``preload`` is the flag to continue training from a preloaded model. If ``feature_extract = False``, the model is\n",
    "finetuned and all model parameters are updated. If\n",
    "``feature_extract = True``, only the last layer parameters are updated,\n",
    "the others remain fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory\n",
    "root_dir = '/home/jupyter/datasets/amigos'\n",
    "\n",
    "# directory with audio\n",
    "data_dir = root_dir + '/audio'\n",
    "\n",
    "# directory with csv\n",
    "csv_dir = root_dir + '/annotations/SelfAsessment'\n",
    "\n",
    "# csv file\n",
    "csv_file = csv_dir + \"/SelfAsessment.csv\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = 'ResNeXt-101-32x8d'\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 4\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "bs = 16\n",
    "\n",
    "# Percentage of data to be used for validation\n",
    "validation_split = .1\n",
    "\n",
    "# Shuffle dataset\n",
    "shuffle_dataset = True\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 10\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "# Flag for reducing size of the training matrix. When False, we do not reduce the size, \n",
    "#   when True wwe reduce the size.\n",
    "reduce = False\n",
    "\n",
    "# Flag to load from a model\n",
    "preload = True\n",
    "\n",
    "# The frequency to be resampled to\n",
    "resample_freq = 8000\n",
    "\n",
    "# number of workers\n",
    "# set to equal the number of cores you\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the Dataset\n",
    "---------------------\n",
    "\n",
    "We will use the AMIGOS dataset to train our network. \n",
    "First, we will look at the csv file that provides information about the\n",
    "individual sound files. ``pandas`` allows us to open the csv file and\n",
    "use ``.iloc()`` to access the data within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading csv\n",
    "csv = pd.read_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train two networks concurrently, one network for arousal and another for valence. Both will classify arousal and valence as either high or low. We will also look at the emotion classes that correspond with the arousal and valence labels: low valence and low arousal (LVLA), low valence and high arousal (LVHA), high valence and low arousal (HVLA), and high valence and high arousal (HVHA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio('/home/jupyter/datasets/amigos/audio/Exp1_P01_audio/P1_10_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio('/home/jupyter/datasets/amigos/audio/Exp1_P01_audio/P1_138_audio.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting the Data\n",
    "-------------------\n",
    "\n",
    "Now that we know the format of the csv file entries, we can construct\n",
    "our dataset. We will create a wrapper class for our dataset using\n",
    "``torch.utils.data.Dataset`` that will handle loading the files and\n",
    "performing some formatting steps. The names of the audio files will be read from the CSV. We will use a 65%/15%/20% train/validation/test spilt. The wrapper\n",
    "class will store the file names, labels, and folder numbers of the audio\n",
    "files in the inputted folder list when initialized. The actual loading\n",
    "and formatting steps will happen in the access function ``__getitem__``.\n",
    "\n",
    "In ``__getitem__``, we use ``torchaudio.load()`` to convert the wav\n",
    "files to tensors. ``torchaudio.load()`` returns a tuple containing the\n",
    "newly created tensor along with the sampling frequency of the audio file\n",
    "(44.1kHz for AMIGOS).  The dataset uses two channels for audio so\n",
    "we will use ``torchaudio.transforms.DownmixMono()`` to convert the audio\n",
    "data to one channel. Next, we need to format the audio data. The network\n",
    "we will make takes an input size of 32,000, while most of the audio\n",
    "files have well over 100,000 samples. The UrbanSound8K audio is sampled\n",
    "at 44.1kHz, so 32,000 samples only covers around 700 milliseconds. By\n",
    "downsampling the audio to aproximately 8kHz, we can represent 4 seconds\n",
    "with the 32,000 samples. This downsampling is achieved by taking every\n",
    "fifth sample of the original audio tensor. Not every audio tensor is\n",
    "long enough to handle the downsampling so these tensors will need to be\n",
    "padded with zeros. The minimum length that wonâ€™t require padding is\n",
    "160,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function pads signals to the target length\n",
    "def pad_signal(signal, target_len):\n",
    "    # inputs: \n",
    "    #    signal: signal to be padded\n",
    "    #    target_len: length to be padded to \n",
    "    # output: \n",
    "    #    padded_signal: size signal.shape[0] x target_len\n",
    "    \n",
    "    len_signal = signal.shape[1]\n",
    "    num_zeros_needed = target_len - signal.shape[1]\n",
    "    padded_signal = torch.zeros(1, target_len)\n",
    "    \n",
    "    if num_zeros_needed > 0:\n",
    "        start_idx = np.random.randint(num_zeros_needed)\n",
    "        padded_signal[:,start_idx:start_idx+len_signal] = signal\n",
    "        return padded_signal\n",
    "    else:\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmigosAudioDataset(Dataset):\n",
    "    \"\"\"Amigos audio dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, data_dir, resample_freq, transform=None, reduce_size=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            data_dir (string): Directory with all the audio.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            reduce_size (bool, optional): Optional flag to reduce the dataset for testing\n",
    "            resample_freq: the frequency for the sound to be resampled at\n",
    "        \"\"\"\n",
    "        # read csv and make a data frame\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if reduce_size:\n",
    "            self.data_frame = df[:33]\n",
    "        else:\n",
    "            self.data_frame = df\n",
    "        self.resample_freq = resample_freq\n",
    "        # store the data directory and transforms as members \n",
    "        self.data_dir = data_dir \n",
    "        self.transform = transform\n",
    "        #initialize lists to hold the path and labels\n",
    "        self.path = []\n",
    "        self.arousal_labels = []\n",
    "        self.valence_labels = []\n",
    "        # find the maximum signal length\n",
    "        self.max_signal_len = 0\n",
    "        # number of faulty files\n",
    "        self.num_faulty_files = 0\n",
    "        # loop through csv entries to build paths\n",
    "        for i in range(0,40):\n",
    "            # get the participant id\n",
    "            participant_id = i+1\n",
    "            if participant_id < 10:\n",
    "                participant_id = \"0{}\".format(participant_id)\n",
    "            else:\n",
    "                participant_id = str(participant_id)\n",
    "            # gather file names\n",
    "            for j in range(0,16):\n",
    "                row_num = (i*16) + (j+1)\n",
    "                audio_id = self.data_frame.iloc[row_num,2]\n",
    "                audio_path = \"{}/Exp1_P{}_audio/P{}_{}_audio.wav\".format(self.data_dir,participant_id,i+1,audio_id[1:-1])\n",
    "                # try to open it\n",
    "                # if it opens find the waveform size\n",
    "                # after that, add its info to the members\n",
    "                try:\n",
    "                    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "                except:\n",
    "                    print(\"Audio file {} is faulty\".format(audio_path))\n",
    "                    self.num_faulty_files += 1\n",
    "                    continue\n",
    "                # record the sample_rate\n",
    "                self.sample_rate = sample_rate\n",
    "                # append the path\n",
    "                self.path.append(audio_path)\n",
    "                # check if the waveform size is larger than the max_signal_len\n",
    "                waveform_size = waveform.shape[1]\n",
    "                if waveform_size > self.max_signal_len:\n",
    "                    self.max_signal_len = waveform_size\n",
    "                # set the labels \n",
    "                arousal = float(self.data_frame.iloc[row_num,4])\n",
    "                valence = float(self.data_frame.iloc[row_num,5])\n",
    "                arousal_label = 0\n",
    "                valence_label = 0\n",
    "                if arousal >= 5:\n",
    "                    arousal_label = 1\n",
    "                if valence >= 5:\n",
    "                    valence_label = 1\n",
    "                self.arousal_labels.append(arousal_label)\n",
    "                self.valence_labels.append(valence_label)\n",
    "        # fix max_signal_len to account for the resampling rate\n",
    "        self.max_signal_len = int(np.ceil(self.max_signal_len/self.sample_rate*self.resample_freq))\n",
    "        print(\"Maximum signal length after downsampling:\",self.max_signal_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame) - self.num_faulty_files - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() \n",
    "        # load audio file\n",
    "        sound, _ = torchaudio.load(self.path[idx], out = None, normalization = True)\n",
    "        # Convert 2 channel audio to 1 channel\n",
    "        sound_mono = torch.mean(sound, dim=0, keepdim=True)\n",
    "        # downsample the audio to the new sample ate\n",
    "        sound_downsamp = torchaudio.transforms.Resample(self.sample_rate, self.resample_freq)(sound[0,:].view(1,-1))\n",
    "        # pad the signal\n",
    "        sound_formatted = pad_signal(sound_downsamp, self.max_signal_len)\n",
    "#         return sound_formatted, self.arousal_labels[index], self.valence_labels[index]\n",
    "#         print(self.arousal_labels[idx])\n",
    "        return sound_formatted, self.arousal_labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Dataset and Dataloader Objects\n",
    "-------------------\n",
    "We now define out dataset and dataloader. We do not have a predefined train/test split, so we will use ``SubsetRandomSampler`` to help do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file /home/jupyter/datasets/amigos/audio/Exp1_P39_audio/P39_10_audio.wav is faulty\n",
      "Maximum signal length after downsampling: 1266640\n",
      "Total number of samples: 639\n"
     ]
    }
   ],
   "source": [
    "dataset =   AmigosAudioDataset(csv_file, data_dir, resample_freq, transform=None, reduce_size=False)\n",
    "dataset_size = len(dataset)\n",
    "print(\"Total number of samples: \" + str(len(dataset)))\n",
    "\n",
    "# Creating data indices for training and validation splits\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "random_seed = 42 # Random seed so we create the same train/val sets\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating data samplers \n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# Creating dataloaders\n",
    "kwargs = {'num_workers': num_workers, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=bs, \n",
    "                                           sampler=train_sampler, **kwargs)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=bs,\n",
    "                                                sampler=valid_sampler, **kwargs)\n",
    "    \n",
    "# kwargs = {'num_workers': num_workers, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "\n",
    "# data_loader = torch.utils.data.DataLoader(dataset, batch_size = bs, shuffle = True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model (M5)\n",
    "-------------------\n",
    "Since we are using raw audio data, we will use, M5, the network architecture described in https://arxiv.org/pdf/1610.00087.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(1, 128, kernel_size=(80,), stride=(4,))\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgPool): AvgPool1d(kernel_size=(1219,), stride=(1219,), padding=(0,))\n",
      "  (fc1): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "M5(\n",
      "  (conv1): Conv1d(1, 128, kernel_size=(80,), stride=(4,))\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgPool): AvgPool1d(kernel_size=(1219,), stride=(1219,), padding=(0,))\n",
      "  (fc1): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M5, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgPool = nn.AvgPool1d(1219) #input should be 512x1219 so this outputs a 512x1\n",
    "        self.fc1 = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim = 2)\n",
    "        return x\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         print(\"pre forward x shape:\", x.shape)\n",
    "#         x = self.conv1(x)\n",
    "#         print(\"post nn.Conv1d(1, 128, 80, 4) x shape:\", x.shape)\n",
    "#         x = F.relu(self.bn1(x))\n",
    "#         print(\"post nn.BatchNorm1d(128) and relu:\", x.shape)\n",
    "#         x = self.pool1(x)\n",
    "#         print(\"post maxpool1(4):\", x.shape)\n",
    "#         x = self.conv2(x)\n",
    "#         print(\"post nn.Conv1d(128, 128, 3):\", x.shape)\n",
    "#         x = F.relu(self.bn2(x))\n",
    "#         print(\"post nn.BatchNorm1d(128) and relu:\", x.shape)\n",
    "#         x = self.pool2(x)\n",
    "#         print(\"post maxpool2(4):\", x.shape)\n",
    "#         x = self.conv3(x)\n",
    "#         print(\"post nn.Conv1d(128, 256, 3):\", x.shape)\n",
    "#         x = F.relu(self.bn3(x))\n",
    "#         print(\"post nn.BatchNorm1d(256) and relu:\", x.shape)\n",
    "#         x = self.pool3(x)\n",
    "#         print(\"post maxpool3(4):\", x.shape)\n",
    "#         x = self.conv4(x)\n",
    "#         print(\"post nn.Conv1d(256, 512, 3):\", x.shape)\n",
    "#         x = F.relu(self.bn4(x))\n",
    "#         print(\"post nn.BatchNorm1d(512) and relu:\", x.shape)\n",
    "#         x = self.pool4(x)\n",
    "#         print(\"post maxpool4(4):\", x.shape)\n",
    "#         x = self.avgPool(x)\n",
    "#         print(\"post avgPool(30) x shape:\", x.shape)\n",
    "#         x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "#         print(\"post permute(0, 2, 1) x shape:\", x.shape)\n",
    "#         x = self.fc1(x)\n",
    "#         print(\"post fc (512,1) x shape:\",x.shape)\n",
    "#         x = F.log_softmax(x, dim = 2)\n",
    "#         print(\"post F.log_softmax(x, dim = 2):\",x.shape)\n",
    "#         return x\n",
    "\n",
    "arousal_model = M5()\n",
    "arousal_model.to(device)\n",
    "valence_model = M5()\n",
    "valence_model.to(device)\n",
    "print(arousal_model)\n",
    "print(valence_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer and scheduler\n",
    "-------------------\n",
    "\n",
    "We will use the same optimization technique used in the paper, an Adam\n",
    "optimizer with weight decay set to 0.0001. At first, we will train with\n",
    "a learning rate of 0.01, but we will use a ``scheduler`` to decrease it\n",
    "to 0.001 during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for arousal\n",
    "arousal_optimizer = optim.Adam(arousal_model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "arousal_scheduler = optim.lr_scheduler.StepLR(arousal_optimizer, step_size = 20, gamma = 0.1)\n",
    "# for valence\n",
    "valence_optimizer = optim.Adam(valence_model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "valence_scheduler = optim.lr_scheduler.StepLR(valence_optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing the Network\n",
    "--------------------------------\n",
    "Now letâ€™s define a training function that will feed our training data\n",
    "into the model and perform the backward pass and optimization steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        arousal_optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "#         print(\"data:\", data)\n",
    "        print(\"data shape:\", data.shape)\n",
    "        target = target.to(device)\n",
    "        data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x1 \n",
    "#         print(\"Output:\", output[0], \"\\nTarget:\",target[0])\n",
    "        print(\"Output shape permute:\", output.shape, \"\\nTarget shape:\",target.shape)\n",
    "        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex1 input\n",
    "        loss.backward()\n",
    "        arousal_optimizer.step()\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a training function, we need to make one for testing\n",
    "the networks accuracy. We will set the model to ``eval()`` mode and then\n",
    "run inference on the test dataset. Calling ``eval()`` sets the training\n",
    "variable in all modules in the network to false. Certain layers like\n",
    "batch normalization and dropout layers behave differently during\n",
    "training so this step is crucial for getting correct results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        pred = output.max(2)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(validation_loader.dataset),\n",
    "        100. * correct / len(validation_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train and test the network. We will train the network\n",
    "for ten epochs then reduce the learn rate and train for ten more epochs.\n",
    "The network will be tested after each epoch to see how the accuracy\n",
    "varies during the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([16, 1, 1266640])\n",
      "Output shape permute: torch.Size([1, 16, 1]) \n",
      "Target shape: torch.Size([16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:29",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-402b0589ecae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First round of training complete. Setting learn rate to 0.001.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marousal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0marousal_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marousal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-89837a86734e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output shape permute:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nTarget shape:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#the loss functions expects a batchSizex1 input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0marousal_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#print training stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:29"
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "for epoch in range(1, 2):\n",
    "    if epoch == 31:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "    train(arousal_model, epoch)\n",
    "    arousal_scheduler.step()\n",
    "    validate(arousal_model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
